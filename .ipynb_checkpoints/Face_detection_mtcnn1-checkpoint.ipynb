{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, Input, MaxPool2D, Reshape, Activation, Flatten, Dense, Permute, PReLU\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import utils\n",
    "import cv2\n",
    "\n",
    "#-----------------------------#\n",
    "#   Get the face frame roughly\n",
    "#   Output bbox location and whether there is a face\n",
    "#-----------------------------#\n",
    "def create_Pnet(weight_path):\n",
    "    input = Input(shape=[None, None, 3])\n",
    "\n",
    "    x = Conv2D(10, (3, 3), strides=1, padding='valid', name='conv1')(input)\n",
    "    x = PReLU(shared_axes=[1,2],name='PReLU1')(x)\n",
    "    x = MaxPool2D(pool_size=2)(x)\n",
    "    \n",
    "    x = Conv2D(16, (3, 3), strides=1, padding='valid', name='conv2')(x)\n",
    "    x = PReLU(shared_axes=[1,2],name='PReLU2')(x)\n",
    "    \n",
    "    x = Conv2D(32, (3, 3), strides=1, padding='valid', name='conv3')(x)\n",
    "    x = PReLU(shared_axes=[1,2],name='PReLU3')(x)\n",
    "    \n",
    "    classifier = Conv2D(2, (1, 1), activation='softmax', name='conv4-1')(x)\n",
    "    \n",
    "    bbox_regress = Conv2D(4, (1, 1), name='conv4-2')(x)\n",
    "\n",
    "    model = Model([input], [classifier, bbox_regress])\n",
    "    model.load_weights(weight_path, by_name=True)\n",
    "    \n",
    "    return model\n",
    "\n",
    "#-----------------------------#\n",
    "#   The third paragraph of mtcnn\n",
    "#   Refine the frame and get five points\n",
    "#-----------------------------#\n",
    "def create_Onet(weight_path):\n",
    "    input = Input(shape = [48,48,3])\n",
    "    \n",
    "    x = Conv2D(32, (3, 3), strides=1, padding='valid', name='conv1')(input)\n",
    "    x = PReLU(shared_axes=[1,2],name='prelu1')(x)\n",
    "    x = MaxPool2D(pool_size=3, strides=2, padding='same')(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), strides=1, padding='valid', name='conv2')(x)\n",
    "    x = PReLU(shared_axes=[1,2],name='prelu2')(x)\n",
    "    x = MaxPool2D(pool_size=3, strides=2)(x)\n",
    "     \n",
    "    x = Conv2D(64, (3, 3), strides=1, padding='valid', name='conv3')(x)\n",
    "    x = PReLU(shared_axes=[1,2],name='prelu3')(x)\n",
    "    x = MaxPool2D(pool_size=2)(x)\n",
    "    \n",
    "    x = Conv2D(128, (2, 2), strides=1, padding='valid', name='conv4')(x)\n",
    "    x = PReLU(shared_axes=[1,2],name='prelu4')(x)\n",
    "    \n",
    "    x = Permute((3,2,1))(x)\n",
    "\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    x = Dense(256, name='conv5') (x)\n",
    "    x = PReLU(name='prelu5')(x)\n",
    "\n",
    "    \n",
    "    classifier = Dense(2, activation='softmax',name='conv6-1')(x)\n",
    "    \n",
    "    bbox_regress = Dense(4,name='conv6-2')(x)\n",
    "    \n",
    "    landmark_regress = Dense(10,name='conv6-3')(x)\n",
    "\n",
    "    model = Model([input], [classifier, bbox_regress, landmark_regress])\n",
    "    model.load_weights(weight_path, by_name=True)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pnet = create_Pnet('pnet.h5')\n",
    "\n",
    "Onet = create_Onet('onet.h5')\n",
    "Onet.save('my_onet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import calculateScales\n",
    "def detectFace( img, threshold): \n",
    "  #-----------------------------#\n",
    "  #  Normalized\n",
    "  #-----------------------------#\n",
    "  copy_img = (img.copy() - 127.5) / 127.5\n",
    "  origin_h, origin_w, _ = copy_img.shape\n",
    "    #-----------------------------#\n",
    "    #   Calculate the original input image\n",
    "    #   The ratio of each zoom\n",
    "    #-----------------------------#\n",
    "  scales = utils.calculateScales(img)\n",
    "  out = []\n",
    "    #-----------------------------#\n",
    "    #   Roughly calculate the face frame\n",
    "    #   pnet part\n",
    "    #-----------------------------#\n",
    "  for scale in scales:\n",
    "    hs = int(origin_h * scale)\n",
    "    ws = int(origin_w * scale)\n",
    "    scale_img = cv2.resize(copy_img, (ws, hs))\n",
    "    inputs = scale_img.reshape(1, *scale_img.shape)\n",
    "    #ouput = self.Pnet.predict(inputs)\n",
    "    ouput = Pnet.predict(inputs)\n",
    "    out.append(ouput)\n",
    "\n",
    "  image_num = len(scales)\n",
    "  rectangles = []\n",
    "  for i in range(image_num):\n",
    "    # Probability of face\n",
    "    cls_prob = out[i][0][0][:,:,1]\n",
    "    #print(cls_prob.shape)\n",
    "    \n",
    "    # The position of its corresponding box \n",
    "    roi = out[i][1][0]\n",
    "    #print(roi.shape)\n",
    "    \n",
    "    # Take out the length and width of each zoomed picture \n",
    "    out_h, out_w = cls_prob.shape\n",
    "    out_side = max(out_h, out_w)\n",
    "    #print(cls_prob.shape)\n",
    "    \n",
    "    # Decoding process      \n",
    "    rectangle = utils.detect_face_12net(cls_prob, roi, out_side, 1 / scales[i], origin_w, origin_h, 0.7)\n",
    "    rectangles.extend(rectangle)\n",
    "\n",
    "  # Non-maximum suppression \n",
    "  rectangles = utils.NMS(rectangles, 0.7)\n",
    "  \n",
    "  if len(rectangles) == 0:\n",
    "    return rectangles\n",
    "\n",
    "  #-----------------------------#\n",
    "  #   Calculating face frame\n",
    "  #   onet part\n",
    "  #-----------------------------#     \n",
    "  predict_batch = []\n",
    "  for rectangle in rectangles:\n",
    "    crop_img = copy_img[int(rectangle[1]):int(rectangle[3]), int(rectangle[0]):int(rectangle[2])]\n",
    "    scale_img = cv2.resize(crop_img, (48, 48))\n",
    "    predict_batch.append(scale_img)\n",
    "\n",
    "  predict_batch = np.array(predict_batch)\n",
    "  #output = self.Onet.predict(predict_batch)\n",
    "  output = Onet.predict(predict_batch)\n",
    "  cls_prob = output[0]\n",
    "  roi_prob = output[1]\n",
    "  pts_prob = output[2]\n",
    "\n",
    "  rectangles = utils.filter_face_48net(cls_prob, roi_prob, pts_prob, rectangles, origin_w, origin_h, 0.7)\n",
    "\n",
    "  return rectangles        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "img = cv2.imread('iacocca_2.jpg')\n",
    "output = detectFace( img, 0.7)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x,y,x1,y1=int(output[0][0]),int(output[0][1]),int(output[0][2]),int(output[0][3])\n",
    "image = cv2.imread('iacocca_2.jpg')\n",
    "image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "image = cv2.rectangle(image, (x,y), (x1,y1), (255, 255, 0), 3)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    " \n",
    "while(True):\n",
    "    ret, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame,1)\n",
    "    output = detectFace(gray, 0.7)\n",
    "    if output == []:\n",
    "        print(\"No face found\")\n",
    "    elif isinstance(output[0], list) :\n",
    "        x,y,x1,y1=int(output[0][0]),int(output[0][1]),int(output[0][2]),int(output[0][3])\n",
    "        gray = cv2.rectangle(gray, (x,y), (x1,y1), (255, 255, 0), 3)\n",
    "    cv2.imshow('frame',gray)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Code to convert model to tflite**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to load the saved .h5 model\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "new_model = tf.keras.models.load_model('my_onet.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the above loaded model into tflite\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(new_model)\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the TF Lite model.\n",
    "with tf.io.gfile.GFile('onet.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Checking the tflite model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the input and output shape of the tflite converted model\n",
    "\n",
    "tflite_interpreter = tf.lite.Interpreter(model_path='onet.tflite')\n",
    "\n",
    "input_details = tflite_interpreter.get_input_details()\n",
    "output_details = tflite_interpreter.get_output_details()\n",
    "\n",
    "print(\"== Input details ==\")\n",
    "print(\"shape:\", input_details[0]['shape'])\n",
    "print(\"type:\", input_details[0]['dtype'])\n",
    "print(\"\\n== Output details ==\")\n",
    "print(\"shape:\", output_details[0]['shape'])\n",
    "print(\"type:\", output_details[0]['dtype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"onet.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test the model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "print(input_shape)\n",
    "image = cv2.imread('iacocca_2.jpg')\n",
    "image = cv2.resize(img,(48,48))\n",
    "print(image.shape)\n",
    "image = np.expand_dims(image,0)\n",
    "input_data = np.array(image,dtype=np.float32)\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "interpreter.invoke()\n",
    "\n",
    "# The function `get_tensor()` returns a copy of the tensor data.\n",
    "# Use `tensor()` in order to get a pointer to the tensor.\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(output_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
